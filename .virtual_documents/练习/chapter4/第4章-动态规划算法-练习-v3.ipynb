import copy





# 咒语：X表示列，Y表示行
class CliffWalkingEnv:
    def __init__(self, ncol=12, nrow=4):
        self.ncol=ncol    # 
        self.nrow = nrow  # 

        self.P = self.createP()

    def createP(self):
        # 每个位置都是一个4元祖, 总共有nrow*ncol个位置
        # 每个元素表示：p, nextstate, reward, done
        # 有了P之后，就可以直到任意点位，的动作空间，以及动作空间对应的奖励等
        # attention: 虽然定义过程用到了i, j行列等属性，但是存储时候是按照压扁等一位坐标存储等
        # 位置一第一维, action是第二维
        
        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]

        # 分别表示上下左右， 例如y-1，则表示向上一行一行
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]

        for i in range(self.nrow):
            for j in range(self.ncol):
                for a in range(4):
                    # 最下面一行，并且不是出发点（等价于在终点或者掉落悬崖）
                    if i == self.nrow-1 and j>0:
                        # i*self.ncol + j表示当前的位置，a表示当前的动作
                        # p, 维持在原地, 奖励=0, 终止符号=True
                        P[i*self.ncol + j][a] = [(1, i*self.ncol+j, 0, True)]
                        continue
                        
                    # other position
                    # here min/max use for valid position
                    next_x = min(self.ncol-1, max(0, j+change[a][0]))  # x坐标是由 「列数j」 来定义的
                    next_y = min(self.nrow-1, max(0, i+change[a][1]))  # y坐标是由 「行数i」 来定义的
                    next_state = next_y * self.ncol + next_x # 这里是相对于远点的位移
                    reward = -1   # default -1
                    done = False  # defalt False

                    # next state is done
                    if next_y == self.nrow-1 and next_x > 0:
                        done = True
                        # 是最后一行 并且不是终点
                        if next_x != self.ncol-1:
                            reward=-100
                    P[i*self.ncol+j][a] = [(1, next_state, reward, done)]
        return P


cliffWalkingEnv = CliffWalkingEnv(nrow=4, ncol=6)
# 输出最后一列作为示例
for p in range(cliffWalkingEnv.nrow * cliffWalkingEnv.ncol):
    if ((p+1) % cliffWalkingEnv.ncol) != 0:continue
    for a in range(cliffWalkingEnv.nrow):

        print('当前位置:pos={},action={}:'.format(p, a), cliffWalkingEnv.P[p][a])





### 策略迭代算法
class PolicyIteration:
    def __init__(self, env, theta, gamma):
        self.env = env

        # 状态价值预估
        self.v = [0 for _ in range(self.env.ncol * self.env.nrow)]

        # 每个position的 转移概率初始化
        self.pi = [[0.25, 0.25, 0.25, 0.25] for i in range(self.env.ncol * self.env.nrow)]
        
        self.theta = theta  # 判断是否收敛的阈值
        self.gamma = gamma  # 衰减因子
        self.action_meaning = ['^', 'v', '<', '>']
        
    def plot_value(self, cnt):
        print("\nstep={}: ".format(cnt))
        for i in range(self.env.nrow):
            for j in range(self.env.ncol):
                print('%6.6s' % ('%.3f' % self.v[i*agent.env.ncol+j]), end=' ')
            print()
    
    # 策略评估
    def policy_evaluation(self):
        # 函数功能 在当前的概率分布下 调整每个状态的价值直到收敛
        # 问题：为什么需要多次迭代才能收敛？
        print('基于循环动规的方式 进行本epoch的 状态评估...')
        totalPos = self.env.ncol * self.env.nrow
        # eval 圈数
        cnt = 0
        while True:
            max_diff = 0  # 维护最大的 状态价值 差异
            new_v = [0] * totalPos  # 初始化价值列表
            for s in range(totalPos):
                qsa_list = []  # 状态粒度， 计算所有Q(s, a)价值
                for a in range(4):
                    qsa = 0  # 动作价值初始化
                    # 实际上, 这里只有一个结果
                    for res in self.env.P[s][a]:
                        p, next_state, r, done = res
                        # 即时奖励 + 后续奖励
                        qsa += p * (r + self.gamma * self.v[next_state] * (1-done))
                    qsa_list.append(self.pi[s][a] * qsa)  # 加权求和 表征 state的价值
                new_v[s] = sum(qsa_list)
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))

            # attention 这里真的修改了状态价值预估
            self.v = new_v
            if max_diff < self.theta: break
            
            if cnt % 10 == 0:
                self.plot_value(cnt+1)
                
            cnt += 1
            break  # 只评估一次
            
        print("策略评估进行{}轮之后收敛\n".format(cnt))

        
    def plot_policy(self):
        print("本轮策略为: ")
        disaster = list(range((self.env.nrow-1) * self.env.ncol+1, 
                    self.env.nrow * self.env.ncol-1))  # 按道理开区间不用减1
        end = [self.env.nrow * self.env.ncol-1]

        for i in range(self.env.nrow):
            for j in range(self.env.ncol):
                position = i*self.env.ncol + j
                if position in disaster:
                    # 悬崖
                    print('****', end=' ')
                elif position in end:
                    # 终点
                    print('EEEE', end=' ')
                else:
                    a = self.pi[position]
                    pi_str = ''
                    for k in range(len(self.action_meaning)):
                        # 基于概率表，如果采取某个动作则输出，否则用o代替
                        pi_str += self.action_meaning[k] if a[k] > 0 else 'o'
                    print(pi_str, end=' ')
            print()
            
    # 策略提升
    def policy_improvement(self):
        # 功能：评估每个位置的后继价值, 从而调整 每个位置的概率分布(只给价值最大的后继分配概率)

        print("开始策略提升, 即调整概率分布...")
        totalPos = self.env.ncol * self.env.nrow
        for s in range(totalPos):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for res in self.env.P[s][a]:
                    p, next_state, r, done = res
                    
                    qsa += p * (r + self.gamma * self.v[next_state] * (1-done))

                qsa_list.append(qsa)
            maxq = max(qsa_list)  # 当前状态的最大Q
            cntq = qsa_list.count(maxq)

            # 最大Q均分概率,其他动作概率为0，这里是策略收敛的核心
            self.pi[s] =[1/cntq if q == maxq else 0 for q in qsa_list]
        print("策略提升完成", end=', ')

        self.plot_policy()
        return self.pi

    
    # 策略执行
    def policy_iteration(self):
        # 交替执行评估和提升,直到概率不再改变
        epoch = 1
        while True:
            print('--------------------------------- epoch={} --------------------------------->'.format(epoch))
            self.policy_evaluation()
            old_pi = copy.deepcopy(self.pi)
            new_pi = self.policy_improvement()
            if old_pi == new_pi:
                break
            epoch += 1



def print_agent(agent, action_meaning, disaster=[], end=[]):
    # 最终的状态价值预估
    print("\n最终的状态价值预估")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            print('%6.6s' % ('%.3f' % agent.v[i*agent.env.ncol+j]), end=' ')
        print()

    print("\n最终策略: ")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            position = i*agent.env.ncol + j
            if position in disaster:
                # 悬崖
                print('****', end=' ')
            elif position in end:
                # 终点
                print('EEEE', end=' ')
            else:
                a = agent.pi[position]
                pi_str = ''
                for k in range(len(action_meaning)):
                    # 基于概率表，如果采取某个动作则输出，否则用o代替
                    pi_str += action_meaning[k] if a[k] > 0 else 'o'
                print(pi_str, end=' ')
        print()








env = CliffWalkingEnv(nrow=4, ncol=8)
action_meaning = ['^', 'v', '<', '>']
theta = 0.001
gamma = 0.9
agent = PolicyIteration(env, theta, gamma)
agent.policy_iteration()









